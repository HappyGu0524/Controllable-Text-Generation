# Controllable Text Generation

![](https://img.shields.io/badge/Status-building-brightgreen)

## Generative Models
### VAE
1. **Extracting and composing robust features with denoising autoencoders** *Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol* `ICML08` [[PDF]](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf) ![](https://img.shields.io/badge/DAE-MLP-red)
2. **Auto-Encoding Variational Bayes** *Diederik P Kingma, Max Welling* `ICLR14` [[PDF]](https://arxiv.org/pdf/1312.6114.pdf) ![](https://img.shields.io/badge/VAE-RNN-red)
3. **Generating Sentences from a Continuous Space** *Samuel R. Bowman, Luke Vilnis* `CONLL16` [[PDF]](https://arxiv.org/pdf/1511.06349.pdf) ![](https://img.shields.io/badge/VAE-RNN-red)
4. **Adversarial Autoencoders** *Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey* `ICLR16` [[PDF]](https://arxiv.org/pdf/1511.05644.pdf) ![](https://img.shields.io/badge/AAE-RNN-red)
5. **Learning Structured Output Representation using Deep Conditional Generative Models** *Kihyuk Sohn, Xinchen Yan, Honglak Lee* `NIPS15` [[PDF]](https://papers.nips.cc/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf) ![](https://img.shields.io/badge/CVAE-CNN-red)
6. **Deep Unsupervised Clustering with Gaussian Mixture Variational Auotoencoders** *Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, Hugh Salimbeni, Kai Arulkumaran, Murray Shanahan* `ICLR17` [[PDF]](https://arxiv.org/pdf/1611.02648.pdf) ![](https://img.shields.io/badge/VAEGMM-RNN-red)
7. **Wasserstein Auto-Encoders** *Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Sch¨olkopf* `ICLR18` [[PDF]](https://arxiv.org/pdf/1711.01558.pdf) [[code]](https://github.com/KyleGoyette/WassersteinAutoEncoder) ![](https://img.shields.io/badge/WAE-CNN-red)
8. **Adversarially Regularized Autoencoders** *Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, Yann LeCun* `ICML18` [[PDF]](https://arxiv.org/pdf/1706.04223v3.pdf) [[code]](https://github.com/jakezhaojb/ARAE) ![](https://img.shields.io/badge/ARAE-RNN-red) <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210318194000.png" width="60%" title="Replace KL divergence with adversarial classifier" align="middle" /> </details> <details> <summary><mark>algorithm</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210318194045.png" width="60%" align="middle" title="1.Reconstruction Loss; 2.Train Classifier; 3.Train Generator" /> </details>
9. **T-CVAE: Transformer-Based Conditioned Variational Autoencoder for Story Completion** *Tianming Wang, Xiaojun Wan* `IJCAI19` [[PDF]](https://www.ijcai.org/proceedings/2019/0727.pdf) [[code]](https://github.com/sodawater/T-CVAE) ![](https://img.shields.io/badge/CVAE-Transformer-red)<details> <summary><mark>abstract</mark></summary> Story completion is a very challenging task of generating the missing plot for an incomplete story, which requires not only understanding but also inference of the given contextual clues. In this paper, we present a novel conditional variational autoencoder based on Transformer for missing plot generation. Our model uses shared attention layers for encoder and decoder, which make the most of the contextual clues, and a latent variable for learning the distribution of coherent story plots. Through drawing samples from the learned distribution, diverse reasonable plots can be generated. Both automatic and manual evaluations show that our model generates better story plots than stateof-the-art models in terms of readability, diversity and coherence.</details> <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210318194427.png" width="60%" align="middle" /> </details>
10. **Transformer-based Conditional Variational Autoencoder for Controllable Story Generation** *Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen* `arXiv` [[PDF]](https://arxiv.org/pdf/2101.00828v1.pdf) [[code]](https://github.com/fangleai/TransformerCVAE) ![](https://img.shields.io/badge/CVAE-Transformer-red)<details> <summary><mark>abstract</mark></summary> We investigate large-scale latent variable models (LVMs) for neural story generation—an under-explored application for open-domain long text—with objectives in two threads: generation effectiveness and controllability. LVMs, especially the variational autoencoder (VAE), have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently, Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning, thus lack satisfying controllability in generation. In this paper, we advocate to revive latent variable modeling, essentially the power of representation learning, in the era of Transformers to enhance controllability without hurting state-of-the-art generation effectiveness. Specifically, we integrate latent representation vectors with a Transformer-based pre-trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder, decoder and the variational posterior are all built on top of pre-trained language models—GPT2 specifically in this paper. Experiments demonstrate state-of-the-art conditional generation ability of our model, as well as its excellent representation learning capability and controllability.</details> <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210318194813.png" width="60%" align="middle" /> </details>
11. **APo-VAE: Text Generation in Hyperbolic Space** *Shuyang Dai, Zhe Gan, Yu Cheng, Chenyang Tao, Lawrence Carin, Jingjing Liu* `NAACL21` [[PDF]](https://arxiv.org/pdf/2005.00054.pdf) [code] ![](https://img.shields.io/badge/APoVAE-RNN-red)

### GAN

### Normalizing Flow
1. **NICE: Non-linear Independent Components Estimation** *Laurent Dinh, David Krueger, Yoshua Bengio* `arXiv` [[PDF]](https://arxiv.org/pdf/1410.8516.pdf)
2. **Density estimation using Real NVP** *Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio* `arXiv` [[PDF]](https://arxiv.org/pdf/1605.08803.pdf)
3. **Glow: Generative Flow with Invertible 1×1 Convolutions** `NeurIPS18` *Diederik P. Kingma, Prafulla Dhariwal* [[PDF]](https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf) [[code]](https://github.com/openai/glow)
4. **Masked Autoregressive Flow for Density Estimation** `NeurIPS17` *George Papamakarios, Theo Pavlakou, Iain Murray* [[PDF]](https://proceedings.neurips.cc/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf)
5. **Improved Variational Inference with Inverse Autoregressive Flow** `NeurIPS16` *Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen*
6. **FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow** `EMNLP2019` *Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, Eduard Hovy* [[PDF]](https://aclanthology.org/D19-1437.pdf) [[code]](https://github.com/XuezheMax/flowseq)
7. **Continuous Language Generative Flow** `ACL21` *Zineng Tang, Shiyue Zhang, Hyounghun Kim, Mohit Bansal* [[PDF]](https://aclanthology.org/2021.acl-long.355.pdf) [[code]](https://github.com/zinengtang/ContinuousFlowNLG)

### Denoising Diffusion Model
1. 

### Language Model


## Controlling information
### Text as exemplar
#### Unparallel text
1. **Disentangled Representation Learning for Non-Parallel Text Style Transfer** *Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova* `ACL19` [[PDF]](https://arxiv.org/pdf/1808.04339.pdf) [[code]](https://github.com/vineetjohn/linguistic-style-transfer) ![](https://img.shields.io/badge/Yelp&Amazon-Style%20Transfer-orange) <details> <summary><mark>motivation</mark></summary> To tackle the problem of disentangling the latent representations of style and content in language models.</details> <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210411113202.png" width="80%" align="middle" /> </details>
2. **Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders** *Yu Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li* `ACL20` [[PDF]](https://arxiv.org/pdf/1911.03882.pdf) [[code]](https://github.com/WHUIR/PPVAE) ![](https://img.shields.io/badge/Yelp&NewsTitle-Controllable%20Generation-orange) <details> <summary><mark>motivation</mark></summary> Flexible when new conditions added to a well trained VAE which requires no more retraining.</details> <details> <summary><mark>abstract</mark></summary> Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.</details> <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210408195017.png" width="80%" title="a) PretrainVAE; b) PluginVAE for each style; c) sample and generate" align="middle" /> </details>
3. **Plug and Play Autoencoders for Conditional Text Generation** *Florian Mai, Nikolaos Pappas, Ivan Montero, Noah A. Smith, James Henderson* `EMNLP20`[[PDF]](https://arxiv.org/pdf/2010.02983.pdf) [[code]](https://github.com/florianmai/emb2emb) ![](https://img.shields.io/badge/Yelp-Controllable%20Generation-orange)  <details> <summary><mark>motivation</mark></summary> Reduce the need for labeled training data for the task and makes the training procedure more efficient by learning a mapping within the autoencoder’s embedding space.<br> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210410153238.png" width="60%" title="The manifold of a text autoencoder is the low-dimensional region of the high-dimensional embedding space where texts are actually embedded." align="middle" /></details> <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210410152622.png" width="80%" title="a) PretrainVAE; b) Training a mapping function; c) Map manifold from input to output" align="middle" /> </details>


3. **Stylized Dialogue Response Generation Using Stylized Unpaired Texts** *Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, Minlie Huang* `AAAI 21` [[PDF]](https://arxiv.org/pdf/2009.12719.pdf) [[code]](https://github.com/silverriver/Stylized_Dialog) ![](https://img.shields.io/badge/TCFC-Dialogue%20Generation-orange) <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210406104923.png" width="60%" title="Transformer incorporating style embedding" align="middle" /> </details> <details> <summary><mark>algorithm</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210406105228.png" width="60%" align="middle" title="3-5: Train both forward and backward transformers, forward with style while backward without style; 9-12: Back translation to get synthetic data; 13: Style transfer" /> </details>
4. **Plug and Play Language Models: A Simple Approach to Controlled Text Generation** *Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu* `ICLR20` [[PDF]](https://arxiv.org/pdf/1912.02164.pdf) [[code]](https://github.com/uber-research/PPLM)
5. **Hooks in the Headline: Learning to Generate Headlines with Controlled Styles** *Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits* `ACL20` [[PDF]](https://arxiv.org/pdf/2004.01980v3.pdf) [[code]](https://github.com/jind11/TitleStylist) ![](https://img.shields.io/badge/NYT&CNN&Humor&Romance&Clickbait-Headline%20Generation-orange)  <details> <summary><mark>motivation</mark></summary> Enrich headlines with controlled style options.<br> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210410160152.png" width="60%" align="middle" /></details> <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210410160246.png" width="60%" title="Entangle latent representation of stylied generation and summarization by partially parameter sharing." align="middle" /> </details>


#### Parallel text

### Text signals
1. **CTRL: A Conditional Transformer Language Model for Controllable Generation** *Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher* `arXiv` [[PDF]](https://arxiv.org/pdf/1909.05858.pdf) [[code]](https://github.com/salesforce/ctrl) ![](https://img.shields.io/badge/OpenWebText-Controllable%20Generation-orange) <details> <summary><mark>control code type</mark></summary> <p>Style by domain <p>More complex control codes <p>Triggering specific tasks <p>Zero-shot code-mixing </details>
2. **A Controllable Model of Grounded Response Generation** *Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, Jianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf, Bill Dolan* `AAAI21` [[PDF]](https://arxiv.org/pdf/2005.00613.pdf) [code] ![](https://img.shields.io/badge/Reddit%20conversation-Response%20Generation-orange)  <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210406155858.png" width="60%" title="GPT2 with Inductive Attention" align="middle" /> </details>
3. **Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text** *Nishtha Madaan, Inkit Padhi, Naveen Panwar, Diptikalyan Saha* `AAAI21` [[PDF]](https://arxiv.org/pdf/2012.04698.pdf) [code] ![](https://img.shields.io/badge/DBPedia&AgNews&Yelp-Counterfactual%20Generation-orange)
4. **GEDI: GENERATIVE DISCRIMINATOR GUIDED SEQUENCE GENERATION** *Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani* `arXiv` [[PDF]](https://arxiv.org/pdf/2009.06367.pdf) [[code]](https://github.com/salesforce/GeDi) ![](https://img.shields.io/badge/IMDb-Controllable%20Generation-orange) 
5. **MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models** *Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, Bryan Catanzaro* `EMNLP20` [[PDF]](https://arxiv.org/pdf/2010.00840.pdf) [[code]](https://github.com/NVIDIA/Megatron-LM) ![](https://img.shields.io/badge/ROCStories-Controllable%20Generation-orange) 

### Syntactic Guidance
1. **Syntax-guided Controlled Generation of Paraphrases** *Ashutosh Kumar, Kabir Ahuja, Raghuram Vadapalli, Partha Talukdar* `TACL20` [[PDF]](https://arxiv.org/pdf/2005.08417v1.pdf) [[code]](https://github.com/malllabiisc/SGCP) ![](https://img.shields.io/badge/ParaNMT&QQP-Syntax%20Guided%20Controlled%20Generation-orange) <details> <summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210410162843.png" width="60%" align="middle" /> </details>
2. **Transformer-Based Neural Text Generation with Syntactic Guidance** *Yinghao Li, Rui Feng, Isaac Rehg, Chao Zhang* `arXiv` [[PDF]](https://arxiv.org/pdf/2010.01737v1.pdf) [[code]](https://github.com/Yinghao-Li/GuiGen) ![](https://img.shields.io/badge/ParaNMT-Syntax%20Guided%20Controlled%20Generation-orange) <details><summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210414202208.png" width="40%" align="middle" title="" /> </details> 

### Multi-signals
1. **Plug and Play Language Models: A Simple Approach to Controlled Text Generation** *Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu* `ICLR20` [[PDF]](https://arxiv.org/pdf/1912.02164.pdf) [[code]](https://github.com/uber-research/PPLM) ![](https://img.shields.io/badge/Direct%20Generation-Controllable%20Generation-orange) <details><summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210430141933.png" width="80%" align="middle" title="" />
2. **A Distributional Approach To Controlled Text Generation** *Muhammad Khalifa, Hady Elsahar, Marc Dymetman* `ICLR21` [[PDF]](https://arxiv.org/pdf/2012.11635.pdf) [[code]](https://github.com/naver/gdc) ![](https://img.shields.io/badge/Wikipedia-Controllable%20Generation-orange) <details><summary><mark>model</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210408164750.png" width="40%" align="middle" title="Energy-Based Model" /> </details> <details> <summary><mark>algorithm</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210408163917.png" width="60%" align="middle" title="Reward = P(x)/q(x); q initialized with pretrained gpt2; from Energy-Based Model to Autoregressive Policy"/> </details>
3. **Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes** *Zhiyu Lin, Mark O. Riedl* `arXiv` [[PDF]](https://arxiv.org/pdf/2104.04039.pdf) [[code]](https://github.com/xxbidiao/plug-and-blend) ![](https://img.shields.io/badge/ROCStories-Controllable%20Generation-orange) 

## Plug-in and Play Framework
1. **Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation** *Matt Post, David Vilar* `NAACL18`  [[PDF]](https://www.aclweb.org/anthology/N18-1119.pdf) [code] ![](https://img.shields.io/badge/AutoRegressive%20Generator-Heuristic%20Decoding-pink)
2. **CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling** *Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li* `AAAI19`  [[PDF]](https://arxiv.org/pdf/1811.10996.pdf) [[code]](https://github.com/NingMiao/CGMH) ![](https://img.shields.io/badge/GeneralLM+BiLM-Non--AutoRegressive%20Decoding-pink)
3. **Mask and Infill: Applying Masked Language Model to Sentiment Transfer** *Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, Songlin Hu* `IJCAI19` [[PDF]](https://www.ijcai.org/proceedings/2019/0732.pdf) [code] ![](https://img.shields.io/badge/BERT-Post%20Editing-pink)
4. **Unsupervised Text Style Transfer with Padded Masked Language Models** *Eric Malmi, Aliaksei Severyn, Sascha Rothe* `EMNLP20` [[PDF]](https://www.aclweb.org/anthology/2020.emnlp-main.699.pdf) [code] ![](https://img.shields.io/badge/BERT-Post%20Editing-pink)
5. **LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer** *Machel Reid, Victor Zhong* `ACL21 findings` [[PDF]](https://arxiv.org/pdf/2105.08206.pdf) [[code]](https://github.com/machelreid/lewis) ![](https://img.shields.io/badge/BART&RoBERT-Post%20Editing-pink)
6. **Plug and Play Autoencoders for Conditional Text Generation** *Florian Mai, Nikolaos Pappas, Ivan Montero, Noah A. Smith, James Henderson* `EMNLP20` [[PDF]](https://arxiv.org/pdf/2010.02983.pdf) [[code]](https://github.com/florianmai/emb2emb) ![](https://img.shields.io/badge/VAE-Embedding%20Mapping-pink)
7. **Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders** *Yu Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li* `ACL20` [[PDF]](https://arxiv.org/pdf/1911.03882.pdf) [[code]](https://github.com/WHUIR/PPVAE) ![](https://img.shields.io/badge/VAE-Embedding%20VAE-pink)
8. **Plug and Play Language Models: A Simple Approach to Controlled Text Generation** *Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu* `ICLR20` [[PDF]](https://arxiv.org/pdf/1912.02164.pdf) [[code]](https://github.com/uber-research/PPLM) ![](https://img.shields.io/badge/GPT-Classifier-pink)
9. **GeDi: Generative Discriminator Guided Sequence Generation** *Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani* `arXiv20` [[PDF]](https://arxiv.org/pdf/2009.06367.pdf) [[code]](https://github.com/salesforce/GeDi) ![](https://img.shields.io/badge/GPT-Generative%20Classifier-pink)
10. **Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation** *Damian Pascual, Beni Egressy, Florian Bolli, Roger Wattenhofer* `arXiv20` [[PDF]](https://arxiv.org/pdf/2012.15416.pdf) [[code]](https://github.com/dapascual/DirectedBeamSearch) ![](https://img.shields.io/badge/GPT-Decoding%20Constraint-pink)
11. **Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes** *Zhiyu Lin, Mark Riedl* `arXiv21` [[PDF]](https://arxiv.org/pdf/2104.04039.pdf) [[code]](https://github.com/xxbidiao/plug-and-blend) ![](https://img.shields.io/badge/GPT-Generative%20Classifier-pink)
12. **Plug-and-Play Conversational Models** *Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth Dathathri, Pascale Fung* `EMNLP20 findings` [[PDF]](https://arxiv.org/pdf/2010.04344.pdf) [[code]](https://github.com/andreamad8/PPCM) ![](https://img.shields.io/badge/GPT-Augmentation-pink)
13. **FUDGE: Controlled Text Generation With Future Discriminators** *Kevin Yang, Dan Klein* `NAACL21` [[PDF]](https://arxiv.org/pdf/2104.05218.pdf) [[code]](https://github.com/yangkevin2/naacl-2021-fudge-controlled-generation) ![](https://img.shields.io/badge/GPT-Classifier-pink)
14. **DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts** *Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, Yejin Choi* `ACL21` [[PDF]](https://arxiv.org/pdf/2105.03023v2.pdf) [code] ![](https://img.shields.io/badge/GPT-Generator-pink)
15. **Zero-Shot Controlled Generation with Encoder-Decoder Transformers** *Devamanyu Hazarika, Mahdi Namazifar, Dilek Hakkani-Tur* `arXiv21` [[PDF]](https://arxiv.org/pdf/2106.06411.pdf) [code] ![](https://img.shields.io/badge/Transformer-Control%20Knobs-pink)

  
## Constrained Text Generation
1. **Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search** *Chris Hokamp, Qun Liu* `ACL17` [[PDF]](https://www.aclweb.org/anthology/P17-1141.pdf) [[code]](https://github.com/chrishokamp/constrained_decoding) ![](https://img.shields.io/badge/Enhanced%20Beam%20Search-yellow)
2. **Gradient-guided Unsupervised Lexically Constrained Text Generation** *Lei Sha* `EMNLP20` [[PDF]](https://www.aclweb.org/anthology/2020.emnlp-main.701.pdf) [code] ![](https://img.shields.io/badge/Enhanced%20Beam%20Search-yellow)
3. **CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling** *Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li* `AAAI19`  [[PDF]](https://arxiv.org/pdf/1811.10996.pdf) [[code]](https://github.com/NingMiao/CGMH) ![](https://img.shields.io/badge/Stochastic%20Search-yellow)
4. **Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation** *Matt Post, David Vilar* `NAACL18`  [[PDF]](https://www.aclweb.org/anthology/N18-1119.pdf) [code] ![](https://img.shields.io/badge/Stochastic%20Search-yellow)
5. **Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting** *J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, Benjamin Van Durme* `NAACL19` [[PDF]](https://www.aclweb.org/anthology/N19-1090.pdf) [[code]](https://github.com/awslabs/sockeye) ![](https://img.shields.io/badge/Enhanced%20Beam%20Search-yellow)
6. **Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation** *Damian Pascual, Beni Egressy, Florian Bolli, Roger Wattenhofer* `arXiv20` [[PDF]](https://arxiv.org/pdf/2012.15416.pdf) [[code]](https://github.com/dapascual/DirectedBeamSearch) ![](https://img.shields.io/badge/Enhanced%20Beam%20Search-yellow)
7. **Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced Monte-Carlo Approach** *Maosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue* `EMNLP20 findings` [[PDF]](https://www.aclweb.org/anthology/2020.findings-emnlp.115.pdf) [[code]](https://github.com/Milozms/TSMH) ![](https://img.shields.io/badge/Enhanced%20Beam%20Search-yellow)
8. **POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training** *Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, Bill Dolan* `EMNLP20` [[PDF]](https://www.aclweb.org/anthology/2020.emnlp-main.698.pdf) [[code]](https://github.com/dreasysnail/POINTER) ![](https://img.shields.io/badge/Non--Autoregressive%20Generation-yellow)
  
## Evaluation
1. **Evaluating Style Transfer for Text** *Remi Mir, Bjarke Felbo, Nick Obradovich, Iyad Rahwan* `NAACL19` [[PDF]](https://arxiv.org/pdf/1904.02295.pdf) [[code]](https://github.com/passeul/style-transfer-model-evaluation) ![](https://img.shields.io/badge/EMD-Style%20Transfer-blue)
2. **A Review of Human Evaluation for Style Transfer** *Eleftheria Briakou, Sweta Agrawal, Ke Zhang, Joel Tetreault, Marine Carpuat* `GEM20201` [[PDF]](https://arxiv.org/pdf/2106.04747.pdf) [code] ![](https://img.shields.io/badge/Human%20Evaluation-Style%20Transfer-blue)

## Benchmark
1. **Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen** *Yixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu, Tat-Seng Chua* `ACL20` [[PDF]](https://arxiv.org/pdf/2005.00701.pdf) [[code]](https://srhthu.github.io/expertise-style-transfer/) ![](https://img.shields.io/badge/Style%20Transfer-purple)
2. **StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer** *Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnabás Póczos, Ruslan Salakhutdinov, Louis-Philippe Morency* `NAACL21` [[PDF]](https://arxiv.org/pdf/2104.05196.pdf) [[code]](https://github.com/lvyiwei1/StylePTB/) ![](https://img.shields.io/badge/Controllable%20Generation-purple) <details><summary><mark>DataSet</mark></summary> <img src="https://github.com/HappyGu0524/pic/blob/master/img/20210421200951.png" width="40%" align="middle" title="" /> </details>
